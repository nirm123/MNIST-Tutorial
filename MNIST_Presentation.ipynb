{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Really good notes if you want to get a better understanding of the topics (http://cs231n.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for building network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Libraries for dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Miscellaneous libraries for analysis\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tutorial_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize all layers of model \"\"\"\n",
    "        super(tutorial_model, self).__init__()\n",
    "    \n",
    "        # Convolution Layer 1\n",
    "        self.convolution1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)  \n",
    "        # Convolution Layer 2\n",
    "        self.convolution2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)    \n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.conv_dropout = nn.Dropout()                                                 \n",
    "        \n",
    "        # Fully Connected Layer 1\n",
    "        self.fc1 = nn.Linear(in_features=16*4*4, out_features=30)  \n",
    "        # Fully Connected Layer 2\n",
    "        self.fc2 = nn.Linear(in_features=30, out_features=10)                            \n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)                                                   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Chain all layers together \"\"\"\n",
    "        # input_x -> Convolution 1 -> RELU -> Pooling -> x_1\n",
    "        x = self.pool(F.relu(self.convolution1(x)))    \n",
    "        # x_1 -> Dropout -> x_2\n",
    "        x = self.conv_dropout(x)               \n",
    "        # x_2 -> Convolution 2 -> RELU -> Pooling -> x_3\n",
    "        x = self.pool(F.relu(self.convolution2(x)))    \n",
    "        \n",
    "        # Collapse 3D tensor to 1D\n",
    "        x = x.view(-1, 16*4*4)                            \n",
    "        \n",
    "        # x_3 -> Fully Connected 1 -> x_4 \n",
    "        x = self.fc1(x)\n",
    "        # x_4 -> Fully Connected 2 -> y\n",
    "        y = self.fc2(x)                                \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "The architecture I implemented is a very simple (common introductory). Here is a visualization.\n",
    "\n",
    "![](http://playagricola.com/Kaggle/extract.png)\n",
    "\n",
    "For more information about this architecture and other potential architectures, check out this link (https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist).\n",
    "\n",
    "### Convolution\n",
    "\n",
    "Each channel in a convolution represents a different kernel convolving over the input image. The \"learning\" in a CNN is primarily about changing the weights of these kernels. Here are some visualizations of convolutions.\n",
    "\n",
    "![](https://www.learnopencv.com/wp-content/uploads/2017/11/convolution-example-matrix.gif)\n",
    "\n",
    "\n",
    "![](https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png)\n",
    "\n",
    "\n",
    "This is the equation for determining the dimensions of an image after a convolution. It is useful when doing the math for kernel sizes when developing a network.\n",
    "\n",
    "![](https://miro.medium.com/max/660/1*D47ER7IArwPv69k3O_1nqQ.png)\n",
    "\n",
    "For more information/visualizations, check out this link (https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1).\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "Max Pooling is like a convolution where the stride = kernel size and the value is the max value. Here is a visualization.\n",
    "\n",
    "![](https://miro.medium.com/max/2344/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)\n",
    "\n",
    "### RELU\n",
    "\n",
    "RELU is one of the many activation units used to introduce non-linearity to a network. Non-linearity is what allows a network to model complex functions. RELU is one of the most commonly used activation units. Here is a graph of RELU.\n",
    "\n",
    "![](https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)\n",
    "\n",
    "For more information about activation uints, check out this link (https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is used in order to combat overfitting. Essentially, you randomly choose to ignore certain nodes, and by doing so, you help avoid overfitting. For more information, checkout this link (https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/dropout_layer.html).\n",
    "\n",
    "### Fully Connected Layer\n",
    "\n",
    "It's difficult to explain a Fully connected Layer in a couple lines. A lot of the concepts related to FC layers are important throughought ML. I would encourage you to do your own research to understand the topic. Some possible sources (https://towardsdatascience.com/understanding-neural-networks-19020b758230, https://www.youtube.com/watch?v=aircAruvnKk).\n",
    "\n",
    "To just understand the code, we use Fully Connevted layers in order to convert the features calculated through convolutions into a classification. That is, we take the 4x4x20 matrix produced by the last convolution, flatten it into a vector of dimension 320x1, and then feed it into two fully connected layers, which output 10 values. We can then feed these 10 values to a softmax in order to calculate the probability the input image was a specfic digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for training\n",
    "num_epochs = 3           \n",
    "\n",
    "# Batch Size for training/testing\n",
    "batch_size = 64          \n",
    "\n",
    "# Learning Rate for optimizer\n",
    "learning_rate = 0.01     \n",
    "\n",
    "# Momentum for optimizer\n",
    "momentum = 0.5           \n",
    "\n",
    "# Dimensions of MNIST\n",
    "dim = 28                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for training data\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    transforms.ToTensor(),                            # Convert grayscale image to pytorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)),             # Normalize grayscale data\n",
    "])\n",
    "\n",
    "# Transformation for training data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                            # Convert grayscale image to pytorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)),             # Normalize grayscale data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data\n",
    "trainset = torchvision.datasets.MNIST(root='./files', train=True, download=True, \n",
    "                                      transform=transform_train)\n",
    "\n",
    "# Initialize dataloader for training data\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, \n",
    "                                           num_workers=8)\n",
    "\n",
    "# Download testing Data\n",
    "testset = torchvision.datasets.MNIST(root='./files', train=False, download=False, \n",
    "                                     transform=transform_test)\n",
    "\n",
    "# Initialize dataloader for testing data\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, \n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/05/Examples-from-the-MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model/Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize previously defined model\n",
    "model = tutorial_model()                                               \n",
    "\n",
    "# Definie loss function (Cross Entropy Loss)\n",
    "criterion = nn.CrossEntropyLoss()                                      \n",
    "\n",
    "# Initialize Optimizer (ADAM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)     \n",
    "\n",
    "# Set model to training (updating weights)\n",
    "model.train();                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "This is the mathematical formulation of cross entropy loss.\n",
    "\n",
    "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/35_blog_image_38.png)\n",
    "\n",
    "This provides us a way of measuring the accuracy of our model. For more information about loss functions in general, check out this link (https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html). For more information about cross entropy loss specifically, check out this link (https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a).\n",
    "\n",
    "### ADAM\n",
    "\n",
    "This is the mathematical formulation of the ADAM Optimizer. It is one of the more commonly used optimizers.\n",
    "\n",
    "![](https://i.stack.imgur.com/08VZN.png)\n",
    "\n",
    "Ultimately, most of machine learning is basically an optimization problem. You are trying to minimize the loss functions. Most optimization methods (including ADAM) revolve around calculating the gradient of the loss function and then taking a step to update the weights. For more information about ADAM, check out this link (https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store time to calculate train time\n",
    "start_time = time.time()\n",
    "\n",
    "# Store loss and accuracy data\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "# Train the model\n",
    "# Loop for number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through data in batch sized increments\n",
    "    for batch_idx, (X_train_batch, Y_train_batch) in enumerate(train_loader):\n",
    "        # If trained on all data in epoch, move onto next epoch\n",
    "        if(Y_train_batch.shape[0]<batch_size):\n",
    "            continue\n",
    "\n",
    "        # Forward pass through network\n",
    "        output = model(X_train_batch)                           \n",
    "        # Calculate loss of predictions\n",
    "        curr_loss = criterion(output, Y_train_batch)            \n",
    "        # Store loss\n",
    "        loss.append(curr_loss.item())                           \n",
    "\n",
    "        \n",
    "        # Clear last calculation\n",
    "        optimizer.zero_grad()                                   \n",
    "        # Calculate gradient based on loss\n",
    "        curr_loss.backward()                                    \n",
    "        # Update model weights\n",
    "        optimizer.step()                                        \n",
    "\n",
    "        # Extract model predictions\n",
    "        _, predicted = torch.max(output.data, 1) \n",
    "        # Calculate number of correct predictions\n",
    "        correct = (predicted == Y_train_batch).sum().item()     \n",
    "        # Calculate/store accuracy\n",
    "        accuracy.append(correct/Y_train_batch.size(0))          \n",
    "        \n",
    "        # Intermitently print statistics\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch: ' + str(epoch+1) + '/' + str(num_epochs) + ', Step: ' \n",
    "                  + str(batch_idx+1) + '/' + str(len(train_loader)) + ', Loss: ' \n",
    "                  + str(curr_loss.item()) + ', Accuracy: ' \n",
    "                  + str(correct/Y_train_batch.size(0)*100) + '%')\n",
    "\n",
    "# Store time to calculate train time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print train time\n",
    "print('Run Time: ' + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Set model to testing (constant weights)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Store number of correct/total samples in test data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Loop through test data\n",
    "    for X_test_batch, Y_test_batch in test_loader:\n",
    "        # Forward pass through network\n",
    "        output = model(X_test_batch)  \n",
    "        \n",
    "        # Extract prediction\n",
    "        _, predicted = torch.max(output.data, 1)    \n",
    "        \n",
    "        # Update total number of sample\n",
    "        total += Y_test_batch.size(0)  \n",
    "        \n",
    "        # Update number of correct predictions\n",
    "        correct += (predicted == Y_test_batch).sum().item()     \n",
    "\n",
    "print('Test Accuracy: ' + str((correct/total) * 100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
